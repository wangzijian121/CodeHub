# 设置根日志级别
log4j.rootLogger=INFO, rollingFile, console

# 控制台输出 appender
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.Target=System.out
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} [%-5p] %c{1} - %m%n

# 滚动日志文件 appender 配置
log4j.appender.rollingFile=org.apache.log4j.rolling.RollingFileAppender
log4j.appender.rollingFile.File=logs/application.log                # 日志文件的基础名称
log4j.appender.rollingFile.MaxFileSize=1GB                           # 设置滚动的最大文件大小为 1GB
log4j.appender.rollingFile.MaxBackupIndex=10                         # 保留 10 个滚动日志文件
log4j.appender.rollingFile.RollingPolicy=org.apache.log4j.rolling.TimeBasedRollingPolicy
log4j.appender.rollingFile.RollingPolicy.FileNamePattern=logs/application-%d{yyyy-MM-dd}.%i.log # 按日期和滚动次数命名文件
log4j.appender.rollingFile.layout=org.apache.log4j.PatternLayout
log4j.appender.rollingFile.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} [%-5p] %c{1} - %m%n



## 设置要记录到控制台的所有内容
#log4j.rootCategory=ERROR, console
#log4j.appender.console=org.apache.log4j.ConsoleAppender
#log4j.appender.console.target=System.err
#log4j.appender.console.layout=org.apache.log4j.PatternLayout
#log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
#
## 将 spark-shell 日志级别默认设置为 WARN。运行 spark-shell 时,这个类的日志级别用于覆盖根 Logger 的日志级别，以便用户可以为 shell 和常规 Spark 应用程序设置不同的默认值。
#log4j.logger.org.apache.spark.repl.Main=ERROR
#
## 用于静默过于详细的第三方日志的设置
#log4j.logger.org.sparkproject.jetty=ERROR
#log4j.logger.org.sparkproject.jetty.util.component.AbstractLifeCycle=ERROR
#log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
#log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
#log4j.logger.org.apache.parquet=ERROR
#log4j.logger.parquet=ERROR
#
## SPARK-9183：在支持 Hive 的 SparkSQL 中查找不存在的 UDF 时避免出现烦人消息的设置
#log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
#log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR